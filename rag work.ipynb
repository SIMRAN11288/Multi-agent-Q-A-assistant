{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1783ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "911f004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents(folder):  #exatracting text files from the folder\n",
    "    docs=[]\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder,filename),'r',encoding='utf-8') as f:\n",
    "                docs.append(f.read())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d3b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors & original data saved\n"
     ]
    }
   ],
   "source": [
    "def document_in_blocks(documents,chunk_sizes=200,chunk_overlap=20):#now dividing\n",
    "    #the data extracted above into chuks and storing in a list\n",
    "    split=RecursiveCharacterTextSplitter(chunk_size=chunk_sizes,chunk_overlap=chunk_overlap)\n",
    "    all_blocks=[]\n",
    "    for doc in documents:\n",
    "        all_blocks.extend(split.split_text(doc))\n",
    "    return all_blocks\n",
    "\n",
    "def store_in_faiss_index(blocks,embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model=SentenceTransformer(embedding_model_name)\n",
    "    embed=model.encode(blocks)  #now convering chunks into embeddings(vectorization)\n",
    "    \n",
    "    index=faiss.IndexFlatL2(embed.shape[1])\n",
    "    index.add(embed)\n",
    "    faiss.write_index(index,\"faiss-index\")   #file storing embeddings(vectors)\n",
    "    with open('blocks',\"wb\") as f:     #pickle storing the original file but pickle\n",
    "        pickle.dump(blocks,f)               #stores in binary format therefore write binary\n",
    "    print(\"vectors & original data saved\")                               #is used\n",
    "     \n",
    "documents=documents(\"C:/Users/user/Desktop/c++ programs/RAG Powered Q&A ASSISTANT\")\n",
    "blocks=document_in_blocks(documents)\n",
    "store_in_faiss_index(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06c9a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\Desktop\\c++ programs\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    index=faiss.read_index(\"faiss-index\")\n",
    "    with open(\"blocks\",'rb') as f:\n",
    "        original_data=pickle.load(f)\n",
    "    return index ,original_data\n",
    "\n",
    "import re\n",
    "import math\n",
    "from sympy import sympify\n",
    "\n",
    "\n",
    "def calculation(query:str):#calculation if calculate word found in query\n",
    "    expression = re.findall(r\"[\\d\\.\\+\\-\\*\\/\\(\\)\\^ ]+\", query) #regex for operators\n",
    "    try:\n",
    "        if expression:\n",
    "            result=sympify(expression[0])\n",
    "            return f\"the result is {result}\"\n",
    "    except expression as e:\n",
    "        return f\"errors{str(e)}\"\n",
    "    return \"could not calculate\"\n",
    "\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "import requests\n",
    "\n",
    "def definition_s(query: str):\n",
    "    # Extract the  term(input) \n",
    "    term = query.strip().split()[-1]\n",
    "\n",
    "    # Make a request to the dictionary API\n",
    "    response = requests.get(f\"https://api.dictionaryapi.dev/api/v2/entries/en/{term}\")\n",
    "\n",
    "    if response.status_code == 200:  #validity check\n",
    "        data = response.json()\n",
    "        try:\n",
    "            definition = data[0]['meanings'][0]['definitions'][0]['definition']\n",
    "            return f\"{term.capitalize()}: {definition}\"\n",
    "        except (IndexError, KeyError):\n",
    "            return \"Definition not found in the response.\"\n",
    "    else:\n",
    "        return f\"No definition found for '{term}'.\"\n",
    "\n",
    "# def definition_s(query:str):\n",
    "#         prompted=f\"define the term {query.strip()}\"\n",
    "#         result=generator(prompted,max_length=100,do_sample=True,temperature=0.3,truncation=True)\n",
    "#         return result[0][\"generated_text\"].strip()\n",
    "    \n",
    "def get_query_embedding(query, model):\n",
    "    return model.encode([query])\n",
    "\n",
    "def find_similar_chunks(query_embedding, index, k=3):\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return indices[0]\n",
    "\n",
    "def get_top_chunks(indices, chunks):\n",
    "    return [chunks[i] for i in indices]\n",
    "\n",
    "def answer_from_chunks(chunks, query):\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    prompt = f\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    #response\n",
    "    result = generator(prompt, max_length=300, do_sample=True, temperature=0.7)\n",
    "\n",
    "    #\n",
    "    return result[0][\"generated_text\"].strip()\n",
    "\n",
    "    \n",
    "def detect_words(query: str):\n",
    "    query_lower = query.lower()\n",
    "    if any(word in query_lower for word in [\"calculate\", \"what is\", \"sum\", \"add\", \"subtract\", \"multiply\", \"divide\"]):\n",
    "        return \"calculate\"\n",
    "    elif any(word in query_lower for word in [\"define\", \"meaning of\", \"what does\"]):\n",
    "        return \"define\"\n",
    "    else:\n",
    "        return \"search\"\n",
    "\n",
    "    \n",
    "def process_query(query):\n",
    "    prompt = detect_words(query)\n",
    "\n",
    "    if prompt == \"calculate\":\n",
    "        return calculation(query)\n",
    "\n",
    "    elif prompt == \"define\":\n",
    "        return definition_s(query)\n",
    "\n",
    "    else:\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        index, chunks = load_data()\n",
    "        query_embedding = get_query_embedding(query, model)\n",
    "        top_indices = find_similar_chunks(query_embedding, index)\n",
    "        top_chunks = get_top_chunks(top_indices, chunks)\n",
    "        return answer_from_chunks(top_chunks, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e853700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Use the following context to answer the question:\n",
      "\n",
      "PRODUCT SPECIFICATIONS :Samsung 253 L 3 Star Frost Free Double Door Refrigerator (Model: RT28A3453S8/HL):\n",
      "Total Capacity: 253 Litres (Net: 234 L)\n",
      "\n",
      "Refrigerator: 181 L\n",
      "\n",
      "F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Use the following context to answer the question:\n",
      "\n",
      "10. How is Wipro Cares related to Wipro Foundation?\n",
      "\n",
      "Wipro Foundation FAQs\n",
      "1. How can a non-profit organization apply for support from Wipro Foundation?\n",
      "\n",
      "Stabilizer-Free\n",
      "Quiting\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query=input(\"ENTER YOUR QUERY\")\n",
    "    if query.lower() in['exit','terminate','quit',\"stop\"]:\n",
    "        print(\"Quiting\")\n",
    "        break\n",
    "    answer=process_query(query)\n",
    "    print(\"\\n\",answer[:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
